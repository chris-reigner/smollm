{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "965c2b93-24e6-47df-ab72-2a89379eeb0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:06:32.288944Z",
     "iopub.status.busy": "2025-07-31T19:06:32.288734Z",
     "iopub.status.idle": "2025-07-31T19:06:33.661193Z",
     "shell.execute_reply": "2025-07-31T19:06:33.660681Z",
     "shell.execute_reply.started": "2025-07-31T19:06:32.288921Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.54.1 datasets==4.0.0 trl==0.20.0 peft==0.16.0 torch torchsummary -q "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa82680e-0d2a-422f-8f19-eadeb0474260",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3652604-cad0-4751-becb-02b173d696ca",
   "metadata": {},
   "source": [
    "In this series of notebooks, we'll go through different fine tuning techniques for the SmolLM families models.\n",
    "The supervised fine-tuning (SFT) and PEFT techniques requires a task-specific dataset structured with input-output pairs. Each pair should consist of:\n",
    "\n",
    "- An input prompt\n",
    "- The expected model response\n",
    "- Any additional context or metadata such like chat examples or summarization examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5c9dd5-e2ee-4e64-b331-f98057b97986",
   "metadata": {},
   "source": [
    "First, we need to have import our dependencies and setup our environment.\n",
    "\n",
    "The SmolLM families requires GPU and at least 8GO of VRAM to run fine tuning.\n",
    "\n",
    "Make sure your environment allows to run it. You can run it on google colab for free !\n",
    "However, it might take longer with the free GPU resources.\n",
    "\n",
    "I used the g5.2xlarge instances from Sagemaker. It is quite cheap and more efficient than free colab GPUs. You can check the pricing of all instances here: https://instances.vantage.sh/aws/ec2/g5.2xlarge?currency=USD\n",
    "\n",
    "We will use the transformers library and trl/peft libraries for SFT/PEFT tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d14c3dda-6f81-4e10-835a-89a3a93024b5",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T19:12:09.186624Z",
     "iopub.status.busy": "2025-07-31T19:12:09.186140Z",
     "iopub.status.idle": "2025-07-31T19:12:16.750032Z",
     "shell.execute_reply": "2025-07-31T19:12:16.749516Z",
     "shell.execute_reply.started": "2025-07-31T19:12:09.186604Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:12:14.312048: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753989134.329181   29299 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753989134.334080   29299 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-31 19:12:14.349534: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc4a8b31-4c4d-41ea-b0fd-19f455e03d63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:12:25.732481Z",
     "iopub.status.busy": "2025-07-31T19:12:25.732158Z",
     "iopub.status.idle": "2025-07-31T19:12:25.775635Z",
     "shell.execute_reply": "2025-07-31T19:12:25.775147Z",
     "shell.execute_reply.started": "2025-07-31T19:12:25.732464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is:cuda\n",
      "Type of card: 8. (8 and above does not support flash attention)\n"
     ]
    }
   ],
   "source": [
    "# Make sure the device is cuda\n",
    "\n",
    "device = (\n",
    "\"cuda\"\n",
    "if torch.cuda.is_available()\n",
    "else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f'Device is:{device}')\n",
    "print(f'Type of card: {torch.cuda.get_device_capability()[0]}. (8 and above does not support flash attention)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9488a-de23-48df-8412-e16eb1a9692b",
   "metadata": {},
   "source": [
    "# SmolLM family description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf954f0-ae30-493b-9f9a-c82e179a2eb2",
   "metadata": {},
   "source": [
    "The SmolLM models are REAL open-source models created by Hugging Face.\n",
    "\n",
    "The reasons why I've chosen to use these models as examples are:\n",
    "- it is open-source and I mean entirely :)\n",
    "- there are great blogs and explanations from the HF team (Kudos)\n",
    "- the models are small and that is probably the next generation of models to be used in AI Agents Frameworks\n",
    "\n",
    "You can find more details on this family of models here: https://github.com/huggingface/smollm or on their great blog: https://huggingface.co/blog/smollm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e840dfd-af1d-46d0-96fe-beeacc8eaaa9",
   "metadata": {},
   "source": [
    "# Text generation fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06d7945-531a-4d29-be1d-bd227bfffff4",
   "metadata": {},
   "source": [
    "We'll start by fine-tuning the model for text generation purposes. This is the most straightforward way to do because the LLM is, well, designed for this.\n",
    "\n",
    "I've chosen the *SmolLM2-360M* model to start with as:\n",
    "- SmolLM2-135M as significantly lower accuracy and features\n",
    "- SmolLM2-360M is the medium size of the v2 family\n",
    "- SmolLM2-1.7B is clearly better but does not fit to GPU free collab and I want everyone to be able to follow this\n",
    "\n",
    "The dataset used to create the SmolLM2 models is *smoltalk* (https://huggingface.co/datasets/HuggingFaceTB/smoltalk)\n",
    "\n",
    "We need to choose the subset of data we want to use for this fine-tuning.\n",
    "As we start by generation, we'll choose *'everyday-conversations'*. The full list of available sub-datasets are:\n",
    "'all', 'smol-magpie-ultra', 'smol-constraints', 'smol-rewrite', 'smol-summarize', 'apigen-80k', 'everyday-conversations', 'explore-instruct-rewriting', 'longalign', 'metamathqa-50k', 'numina-cot-100k', 'openhermes-100k', 'self-oss-instruct', 'systemchats-30k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "529585c0-5c58-4c21-91fe-a44d7b0821b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:27:06.338674Z",
     "iopub.status.busy": "2025-07-31T19:27:06.338267Z",
     "iopub.status.idle": "2025-07-31T19:27:06.341664Z",
     "shell.execute_reply": "2025-07-31T19:27:06.341087Z",
     "shell.execute_reply.started": "2025-07-31T19:27:06.338657Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"HuggingFaceTB/SmolLM2-360M\"\n",
    "dataset_name = \"HuggingFaceTB/smoltalk\"\n",
    "model_cache_dir=model_name.split('/')[-1]\n",
    "config_name = \"everyday-conversations\"\n",
    "dataset_cache_dir=f\"{dataset_name.replace('/', '')}_{config_name}\"\n",
    "output_dir = \"./sft_text_generation_360M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae50a0db-b2a5-4ae3-ae8d-5342cf0c8296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:18:48.657463Z",
     "iopub.status.busy": "2025-07-31T19:18:48.656931Z",
     "iopub.status.idle": "2025-07-31T19:19:04.379427Z",
     "shell.execute_reply": "2025-07-31T19:19:04.378816Z",
     "shell.execute_reply.started": "2025-07-31T19:18:48.657445Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72fc561f5240eb85cf65b4d153a0de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d255b8996ef4e6bbaf10ba459901b81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d907d7fdb7243dab05e2627f84639c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60de5126df514bb695cd2aebf8e081b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760dc6e63da84a6ebb60853e8f073653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a4a86e95b34c43a99708dd98558857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ee1ba30f6c494184e22c29c3d0c44a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d113a42b3334c2fbbbc8a567d4130b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the model and tokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "pretrained_model_name_or_path=model_name,\n",
    "cache_dir=model_cache_dir\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "pretrained_model_name_or_path=model_name,\n",
    "cache_dir=model_cache_dir\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b071d2-92ac-420f-8ccf-234f4e3ba82f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:20:06.102584Z",
     "iopub.status.busy": "2025-07-31T19:20:06.102183Z",
     "iopub.status.idle": "2025-07-31T19:20:06.106006Z",
     "shell.execute_reply": "2025-07-31T19:20:06.105473Z",
     "shell.execute_reply.started": "2025-07-31T19:20:06.102567Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 960,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2560,\n",
      "  \"is_llama_config\": true,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 15,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 5,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_interleaved\": false,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 49152\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the model configuration\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0c9cf2-65ea-4cd8-bbdb-da1d4dea2eb6",
   "metadata": {},
   "source": [
    "Great ! The model weights about 1 GO in storage which makes it easy to use.\n",
    "We can see a lot of great information here such like the type of data, the model architecture and family as well as the vocabulary size.\n",
    "\n",
    "We can also, check mode details on the structure of the model using torchsummary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "432823e3-94b5-404a-beca-5c4439007d24",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:48:42.707457Z",
     "iopub.status.busy": "2025-07-31T19:48:42.706965Z",
     "iopub.status.idle": "2025-07-31T19:48:44.371002Z",
     "shell.execute_reply": "2025-07-31T19:48:44.370431Z",
     "shell.execute_reply.started": "2025-07-31T19:48:42.707434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
      "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
      "              ReLU-7        [-1, 128, 112, 112]               0\n",
      "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
      "              ReLU-9        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
      "             ReLU-12          [-1, 256, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-14          [-1, 256, 56, 56]               0\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
      "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [-1, 512, 28, 28]               0\n",
      "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [-1, 512, 28, 28]               0\n",
      "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
      "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [-1, 512, 14, 14]               0\n",
      "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [-1, 512, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [-1, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0\n",
      "           Linear-33                 [-1, 4096]     102,764,544\n",
      "             ReLU-34                 [-1, 4096]               0\n",
      "          Dropout-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 4096]      16,781,312\n",
      "             ReLU-37                 [-1, 4096]               0\n",
      "          Dropout-38                 [-1, 4096]               0\n",
      "           Linear-39                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 218.78\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 747.15\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "vgg = models.vgg16().to(device)\n",
    "\n",
    "summary(vgg, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a10ccd-165b-4b11-b369-84eaf97d8cf5",
   "metadata": {},
   "source": [
    "Note that we apply the setup_chat_format function to our model and tokenizer.\n",
    "This is because we need a specific format for our fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72319f-5e51-4196-8d15-a1bf6d77f6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:40:13.168480Z",
     "iopub.status.busy": "2025-07-31T19:40:13.167978Z",
     "iopub.status.idle": "2025-07-31T19:40:13.172263Z",
     "shell.execute_reply": "2025-07-31T19:40:13.171576Z",
     "shell.execute_reply.started": "2025-07-31T19:40:13.168460Z"
    }
   },
   "source": [
    "Also note that we force the device to *cuda* to make sure the model and the future input vectors will be both served on the same engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "644a866b-9316-4553-b467-9f8be62b196e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:23:23.069202Z",
     "iopub.status.busy": "2025-07-31T19:23:23.068781Z",
     "iopub.status.idle": "2025-07-31T19:23:24.187592Z",
     "shell.execute_reply": "2025-07-31T19:23:24.187093Z",
     "shell.execute_reply.started": "2025-07-31T19:23:23.069184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b798ea998d94f90b58f130acdc3880e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dd986767564901adcbb3caa4aa63c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/119 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 2260\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['full_topic', 'messages'],\n",
       "        num_rows: 119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and store dataset\n",
    "\n",
    "ds = load_dataset(dataset_name, config_name, cache_dir=dataset_cache_dir)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80cc9f7-265b-406b-9527-ed9ba1807326",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:23:41.724463Z",
     "iopub.status.busy": "2025-07-31T19:23:41.723893Z",
     "iopub.status.idle": "2025-07-31T19:23:41.729230Z",
     "shell.execute_reply": "2025-07-31T19:23:41.728657Z",
     "shell.execute_reply.started": "2025-07-31T19:23:41.724440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_topic': 'Cooking/Family recipes/Family recipe storytelling',\n",
       " 'messages': [{'content': 'Hi', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for some new recipes to try out, do you have any family recipes you can share?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"I can share some classic family recipes. How about a simple chicken parmesan recipe that's been passed down through generations?\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"That sounds delicious, what's the story behind the recipe?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"It originated from an Italian grandmother who made it for her family every Sunday. She'd bread the chicken with love and care, and it became a tradition that's been carried on for years.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"That's so sweet, I'd love to try it out and start my own tradition.\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"I'm glad you're excited to try it! I can provide you with the full recipe and instructions if you'd like.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's explore\n",
    "ds['train'][300]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ac09a2-62f3-4e15-90e5-be5598d6121c",
   "metadata": {},
   "source": [
    "# SFT for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc7b40f-ffbd-45ea-82ba-665b72d35427",
   "metadata": {},
   "source": [
    "We'll use the trl libray to fine-tune the model.\n",
    "You can check the details of the SFT Trainer here: https://huggingface.co/docs/trl/en/sft_trainer\n",
    "\n",
    "Hugging face provides some \"recipes\" to configure the fine tuning for SmolLM2: https://github.com/huggingface/alignment-handbook/tree/main/recipes/smollm. We follow as much as possible their recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144393e-6bad-49d3-a394-6f3effe42165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:51:36.203247Z",
     "iopub.status.busy": "2025-07-31T19:51:36.203058Z",
     "iopub.status.idle": "2025-07-31T19:51:36.206905Z",
     "shell.execute_reply": "2025-07-31T19:51:36.206332Z",
     "shell.execute_reply.started": "2025-07-31T19:51:36.203226Z"
    }
   },
   "source": [
    "### TIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d077af8-c7d7-4420-b5c7-8dfb4113fb53",
   "metadata": {},
   "source": [
    "Sometimes the configuration shared from HF or blogs save checkpoints and run evaluations very frequently.\n",
    "\n",
    "Each time you save your model, you're literraly writing it in local storage which takes some time. Checkpointing is important and really relevant for distributed training and huge models. In our case, frugality is the best and you don't need to checkpoint every 100 steps or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acae0179-3f64-466c-9ddb-efaded40313b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:27:14.139639Z",
     "iopub.status.busy": "2025-07-31T19:27:14.139243Z",
     "iopub.status.idle": "2025-07-31T19:27:14.395318Z",
     "shell.execute_reply": "2025-07-31T19:27:14.394741Z",
     "shell.execute_reply.started": "2025-07-31T19:27:14.139621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure trainer\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    max_steps=1000,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=100,\n",
    "    save_steps=300,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "        ),  # Use MPS for mixed precision training\n",
    "    #packing=True, #Only used when flash-attention-2 is used\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9213bdc-6ec0-42d9-8e7f-28cbf5960a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:27:15.757936Z",
     "iopub.status.busy": "2025-07-31T19:27:15.757462Z",
     "iopub.status.idle": "2025-07-31T19:34:10.663634Z",
     "shell.execute_reply": "2025-07-31T19:34:10.662787Z",
     "shell.execute_reply.started": "2025-07-31T19:27:15.757913Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 06:37, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.937843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.030300</td>\n",
       "      <td>0.904401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.030300</td>\n",
       "      <td>0.885333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.874348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>0.860767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.852138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.845400</td>\n",
       "      <td>0.844574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.840862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.826000</td>\n",
       "      <td>0.835543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.826222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.831200</td>\n",
       "      <td>0.821039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.712400</td>\n",
       "      <td>0.840357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.712400</td>\n",
       "      <td>0.834355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>0.837898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.569400</td>\n",
       "      <td>0.835681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.833465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.830261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>0.829323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>0.828260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.555400</td>\n",
       "      <td>0.827886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36223842-4a2b-43b8-948d-dddbeccda040",
   "metadata": {},
   "source": [
    "### TIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c58f437-9445-4aa2-8f39-484ec728197c",
   "metadata": {},
   "source": [
    "While the fine tuning is happening, you can follow the consumption of your GPU by running \n",
    "*nvidia-smi* in your terminal\n",
    "\n",
    "![Alt text](./nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515d876a-c0d8-4edc-9373-76ad06c882c8",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b9495-0ee8-4f83-a687-dcdcc660509b",
   "metadata": {},
   "source": [
    "Ok, this was amazingly fast ! We can see that there seems to be some overfitting between our training and evaluation dataset.\n",
    "\n",
    "Actually the evaluation dataset is quite small and these models require more data than we have right now.\n",
    "Yet let's see our results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff09a37f-ffa9-4bb8-b68c-d9f7b47431d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T19:34:52.800236Z",
     "iopub.status.busy": "2025-07-31T19:34:52.799890Z",
     "iopub.status.idle": "2025-07-31T19:34:53.213494Z",
     "shell.execute_reply": "2025-07-31T19:34:53.213028Z",
     "shell.execute_reply.started": "2025-07-31T19:34:52.800219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "user\n",
      "what is the meaning of life ?\n",
      "the meaning of life is to live your life to the fullest.\n"
     ]
    }
   ],
   "source": [
    "# Test the fine-tuned model \n",
    "prompt = \"what is the meaning of life ?\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "print(\"After training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bc95a4-d57e-4ce6-8b99-89169728f76c",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30032aee-7f64-4504-95df-f0f857ee9504",
   "metadata": {},
   "source": [
    "That's great we did our first supervised fine tuning model in 10 mns of time !\n",
    "\n",
    "Try to play with hyper-parameters, change the fine-tuning dataset and see if the results are up to your expectations !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4dee6-3bae-451f-8b07-af880cf636f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# SmolLM learning 

## Objectives
In this series of notebooks, we'll go through different fine tuning techniques for the SmolLM families models.
The supervised fine-tuning (SFT) and PEFT techniques requires a task-specific dataset structured with input-output pairs.

Hugging Face is open-souring a whole family of models: text, vision, multimodal, tools as well as examples on how to apply these models on real datasets!

We focus first on a summarizing task with the smolLM2 series. Find more information here: https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9

Next steps: Finalize evaluation and move to smolLM3 series (I recommend the very good blog post from HF: https://huggingface.co/blog/smollm3)


## Resources for GPU optimization

https://medium.com/@ishita.verma178/pytorch-gpu-optimization-step-by-step-guide-9dead5164ca2
